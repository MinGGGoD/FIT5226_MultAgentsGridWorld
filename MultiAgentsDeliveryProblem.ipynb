{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdbf7334",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56f910cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import numpy.random as npr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21105c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid world environment\n",
    "class GridWorldEnvironment:\n",
    "    def __init__(self, size=5, agents_num=4):\n",
    "        self.size = size\n",
    "        self.agents_num = agents_num\n",
    "        self.agents_positions = {}  # agent position\n",
    "        self.agents_reached_A = {}  # if agents get item\n",
    "        self.A_position = None\n",
    "        self.B_position = (size - 1, size - 1)  # fixed location of B\n",
    "        self.directions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.total_collisions = 0\n",
    "        self.total_steps = 0\n",
    "        self.agents_idx = list(range(agents_num))\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to its initial state.\n",
    "        \"\"\"\n",
    "        # initialize A position\n",
    "        self.A_position = (\n",
    "            npr.randint(0, self.size - 1),\n",
    "            npr.randint(0, self.size - 1),\n",
    "        )\n",
    "        # ensure A and B are not in the same position\n",
    "        while self.A_position == self.B_position:\n",
    "            self.A_position = (\n",
    "                npr.randint(0, self.size - 1),\n",
    "                npr.randint(0, self.size - 1),\n",
    "            )\n",
    "\n",
    "        # initialize agents' positions and reached_A status\n",
    "        self.agents_positions = {}\n",
    "        self.agents_reached_A = {}\n",
    "        for idx in self.agents_idx:\n",
    "            if npr.rand() < 0.5:\n",
    "                self.agents_positions[idx] = self.A_position\n",
    "                self.agents_reached_A[idx] = True\n",
    "            else:\n",
    "                self.agents_positions[idx] = self.B_position\n",
    "                self.agents_reached_A[idx] = False\n",
    "\n",
    "        self.total_collisions = 0\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def _get_destination(self, agent_idx):\n",
    "        \"\"\"\n",
    "        Get the destination position(A or B)\n",
    "        \"\"\"\n",
    "        return \"B\" if self.agents_reached_A[agent_idx] else \"A\"\n",
    "\n",
    "    def _find_nearby_collision_agents(self, agent_id):\n",
    "        \"\"\"\n",
    "        Find nearby agents that might collide.\n",
    "        \"\"\"\n",
    "        y, x = self.agents_positions[agent_id]\n",
    "        destination_cur = self._get_destination(agent_id)\n",
    "        nearby_agents = [\n",
    "            (-1, -1),\n",
    "            (-1, 0),\n",
    "            (-1, 1),\n",
    "            (0, -1),\n",
    "            (0, 1),\n",
    "            (1, -1),\n",
    "            (1, 0),\n",
    "            (1, 1),\n",
    "        ]\n",
    "        collision_status = []\n",
    "        for dy, dx in nearby_agents:\n",
    "            new_y, new_x = y + dy, x + dx\n",
    "            # Check if new position is valid\n",
    "            if 0 <= new_y < self.size and 0 <= new_x < self.size:\n",
    "                collision = False\n",
    "                for other_agent_id in self.agents_idx:\n",
    "                    if (\n",
    "                        other_agent_id != agent_id\n",
    "                        and self.agents_positions[other_agent_id] == (new_y, new_x)\n",
    "                        and self._get_destination(other_agent_id) == destination_cur\n",
    "                    ):  # agents are going to the same destination would cause collision\n",
    "                        collision = 1\n",
    "                collision_status.append(collision)\n",
    "            else:\n",
    "                collision_status.append(0)\n",
    "\n",
    "    def get_state(self, agent_idx):\n",
    "        \"\"\"\n",
    "        Get the state of the environment for a specific agent.\n",
    "        \"\"\"\n",
    "        position = self.agents_positions[agent_idx]\n",
    "        reached_A = self.agents_reached_A[agent_idx]\n",
    "        manhattan_distance_to_A = (self.A_position[0] - position[0]) + (\n",
    "            self.A_position[1] - position[1]\n",
    "        )\n",
    "        manhattan_distance_to_B = (self.B_position[0] - position[0]) + (\n",
    "            self.B_position[1] - position[1]\n",
    "        )\n",
    "        collision_agents = self._find_nearby_collision_agents(agent_idx)\n",
    "\n",
    "        return np.array(\n",
    "            [\n",
    "                position[0],\n",
    "                position[1],\n",
    "                self.A_position[0],\n",
    "                self.A_position[1],\n",
    "                self.B_position[0],\n",
    "                self.B_position[1],\n",
    "                manhattan_distance_to_A[0],\n",
    "                manhattan_distance_to_A[1],\n",
    "                manhattan_distance_to_B[0],\n",
    "                manhattan_distance_to_B[1],\n",
    "                reached_A,\n",
    "                *collision_agents,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _check_done(self, agent_idx):\n",
    "        \"\"\"\n",
    "        Check if the agent has reached its destination.\n",
    "        \"\"\"\n",
    "        if (\n",
    "            self.agents_positions[agent_idx] == self.B_position\n",
    "            and self.agents_reached_A[agent_idx]\n",
    "        ):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def take_action(self, action_dict):\n",
    "        \"\"\"\n",
    "        Take an action in the environment and return the next state, reward and collosions.\n",
    "        \"\"\"\n",
    "        planned_actions = {}  # {action_idx: action}\n",
    "        wall_collisions = []  # number of hitting wall\n",
    "\n",
    "        for agent_idx, action in action_dict.items():\n",
    "            y, x = self.agents_positions[agent_idx]\n",
    "            if self.agents_positions[agent_idx] == \"up\":\n",
    "                new_y, new_x = y - 1, x\n",
    "            elif self.agents_positions[agent_idx] == \"down\":\n",
    "                new_y, new_x = y + 1, x\n",
    "            elif self.agents_positions[agent_idx] == \"left\":\n",
    "                new_y, new_x = y, x - 1\n",
    "            elif self.agents_positions[agent_idx] == \"right\":\n",
    "                new_y, new_x = y, x + 1\n",
    "\n",
    "            # check valid\n",
    "            if 0 <= new_y < self.size and 0 <= new_x < self.size:\n",
    "                planned_actions[agent_idx] = (new_y, new_x)  # move\n",
    "                wall_collisions.append(False)\n",
    "            else:\n",
    "                planned_actions[agent_idx] = (y, x)  # not move\n",
    "                wall_collisions.append(True)\n",
    "\n",
    "        # check collision\n",
    "        next_positions = copy.deepcopy(self.agents_positions)\n",
    "        for idx in self.agents_idx:\n",
    "            next_positions[idx] = planned_actions[idx]\n",
    "\n",
    "        collisions = 0  # number of head-on collisions\n",
    "        position_to_agents = {}\n",
    "        for agent_idx, pos in next_positions.items():\n",
    "            if pos not in position_to_agents:\n",
    "                position_to_agents[pos] = []\n",
    "            position_to_agents[pos].append(agent_idx)\n",
    "\n",
    "        agents_collisions = set()\n",
    "        for pos, agents_cur in position_to_agents.items():\n",
    "            if len(agents_cur) > 1:\n",
    "                dirs = [self._get_destination(agent_idx) for agent_idx in agents_cur]\n",
    "                if \"B\" in dirs and \"A\" in dirs:\n",
    "                    collisions += 1\n",
    "                    agents_collisions.update(agents_cur)\n",
    "\n",
    "        # update agents' positions\n",
    "        self.agents_positions = next_positions\n",
    "\n",
    "        # calculate rewards\n",
    "        rewards = {}\n",
    "        for agent_idx in self.agents_idx:\n",
    "            reward = 0\n",
    "            if wall_collisions[agent_idx]:\n",
    "                reward -= 5  # hitting wall penalty\n",
    "            else:\n",
    "                reward -= 1  # step cost\n",
    "\n",
    "            location = self.agents_positions[agent_idx]\n",
    "            if self.agents_reached_A[agent_idx]:\n",
    "                if location == self.B_position:\n",
    "                    reward += 100  # delivery success\n",
    "                    self.agents_reached_A[agent_idx] = False\n",
    "            else:\n",
    "                if location == self.A_position:\n",
    "                    reward += 50  # pickup success\n",
    "                    self.agents_reached_A[agent_idx] = True\n",
    "\n",
    "        rewards[agent_idx] = reward  # store reward\n",
    "\n",
    "        # accumulate total collisions and steps\n",
    "        self.total_collisions += collisions\n",
    "        self.total_steps += 1\n",
    "\n",
    "        # format next state\n",
    "        next_states = {}\n",
    "        for agent_idx in self.agents_idx:\n",
    "            next_states[agent_idx] = self.get_state(agent_idx)\n",
    "\n",
    "        return next_states, rewards, collisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9484e136",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d4882c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep q-learning agent\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        statespace_size,\n",
    "        action_size,\n",
    "        gamma=0.9,\n",
    "        epsilon=1,\n",
    "        epsilon_decay=0.9995,\n",
    "        min_epsilon=0.1,\n",
    "        batch_size=200,\n",
    "        replay_buffer_size=1000,\n",
    "        lr=0.001,\n",
    "        copy_frequency=100,\n",
    "    ):\n",
    "        self.statespace_size = statespace_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.lr = lr\n",
    "        self.copy_frequency = copy_frequency\n",
    "\n",
    "        self.steps = 0  # count agent's steps\n",
    "        self.replay_buffer = []  # memory\n",
    "        self.replay_buffer_size = replay_buffer_size  # memory size\n",
    "\n",
    "        # initialize the DQN\n",
    "        self.model, self.model2, self.optimizer, self.loss_fn = self.prepare_torch()\n",
    "\n",
    "    def prepare_torch(self):\n",
    "        l1 = self.statespace_size\n",
    "        l2 = 24\n",
    "        l3 = 24\n",
    "        l4 = 4\n",
    "        model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(l1, l2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(l2, l3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(l3, l4),\n",
    "        )\n",
    "        model2 = copy.deepcopy(model)\n",
    "        model2.load_state_dict(model.state_dict())\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self)\n",
    "        return model, model2, optimizer, loss_fn\n",
    "\n",
    "    def update_target(self):\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        state1 = torch.from_numpy(state).float()\n",
    "        qvals_torch = self.model(state1)\n",
    "        qvals = qvals_torch.data.numpy()\n",
    "        return qvals\n",
    "\n",
    "    def get_maxQ(self, s):\n",
    "        return torch.max(self.model2(torch.from_numpy(s).float())).float()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if npr.uniform() < self.epsilon:\n",
    "            action = npr.choice(self.action_size)\n",
    "        else:\n",
    "            qvals = self.get_qvals(state)\n",
    "            action = np.argmax(qvals)\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Store the transition in the replay buffer.\n",
    "        \"\"\"\n",
    "        if len(self.replay_buffer) >= self.replay_buffer_size:\n",
    "            self.replay_buffer.pop(0)\n",
    "        self.replay_buffer.append((state, action, reward, next_state))\n",
    "\n",
    "    def train_one_step(self, states, actions, targets):\n",
    "        # convert the states and actions and targets to tensors\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        targets = np.array(targets)\n",
    "\n",
    "        state_batch = torch.tensor(states, dtype=torch.float32)\n",
    "        action_batch = torch.tensor(actions, dtype=torch.long)\n",
    "        target_batch = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "        # get Q-values for the current states\n",
    "        q_values = self.model(state_batch)\n",
    "        predicted_q_values = q_values.gather(1, action_batch.unsqueeze(1)).squeeze()\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = self.loss_fn(predicted_q_values, target_batch)\n",
    "\n",
    "        # backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the agent using the replay buffer.\n",
    "        \"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return  # samples not enough\n",
    "\n",
    "        # sample a batch from the replay buffer\n",
    "        minibatch = npr.sample(\n",
    "            self.replay_buffer,\n",
    "            self.batch_size,\n",
    "        )\n",
    "        states, actions, rewards, next_states = zip(*minibatch)\n",
    "\n",
    "        # TD targets\n",
    "        targets = []\n",
    "        for i in range(len(minibatch)):\n",
    "            next_maxQ = self.get_maxQ(next_states[i])\n",
    "            action_target = rewards[i] + self.gamma * next_maxQ\n",
    "            targets.append(action_target)\n",
    "\n",
    "        # train the model\n",
    "        loss = self.train_one_step(states, actions, targets)\n",
    "\n",
    "        # update network periodically\n",
    "        self.steps += 1\n",
    "        if self.steps % self.copy_frequency == 0:\n",
    "            self.update_target()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # decay epsilon\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b27cf79",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41721ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "def train_agents(\n",
    "    agent, env, max_steps=1500000, max_collisions=4000, max_walltime=600, verbose=True\n",
    "):\n",
    "    \"\"\"Training each agent in the environment.\"\"\"\n",
    "    # start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # initialize the environment\n",
    "    env._reset()\n",
    "\n",
    "    # initial states\n",
    "    states = {agent_idx: env.get_state(agent_idx) for agent_idx in env.agents_idx}\n",
    "\n",
    "    # global variables\n",
    "    total_collisions = 0\n",
    "    total_steps = 0\n",
    "    episode = 0\n",
    "\n",
    "    while total_collisions <= max_collisions and total_steps <= max_steps:\n",
    "        if time.time() - start_time > max_walltime:\n",
    "            print(\"===== Time limit exceeded. =====\")\n",
    "            break\n",
    "\n",
    "        actions_dict = {}\n",
    "        for agent_idx in env.agents_idx:  # central clock - fix order\n",
    "            action = agent.get_action(states[agent_idx])\n",
    "            actions_dict[agent_idx] = action\n",
    "\n",
    "        # take action in the environment\n",
    "        next_states, rewards, collisions = env.take_action(actions_dict)\n",
    "\n",
    "        # store transition in replay buffer\n",
    "        for agent_idx in env.agents_idx:\n",
    "            state = states[agent_idx]\n",
    "            action = actions_dict[agent_idx]\n",
    "            reward = rewards[agent_idx]\n",
    "            next_state = next_states[agent_idx]\n",
    "            agent.store_transition(state, action, reward, next_state)\n",
    "\n",
    "        # train the agent\n",
    "        if len(agent.replay_buffer) >= agent.batch_size:\n",
    "            loss = agent.train()\n",
    "\n",
    "        # update the states\n",
    "        states = next_states\n",
    "\n",
    "        total_collisions += collisions\n",
    "        total_steps += 1\n",
    "        episode += 1\n",
    "\n",
    "        agent.decay_epsilon()\n",
    "        if verbose and total_steps % 10000 == len(env.agents_idx):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(\n",
    "                f\"Steps: {total_steps}/{max_steps}, Collisions: {total_collisions}/{max_collisions}, Epsilon: {agent.epsilon:.3f}, Time Elapsed: {elapsed_time:.2f}s\"\n",
    "            )\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "    print(f\"Total steps: {total_steps}\")\n",
    "    print(f\"Total collisions: {total_collisions}\")\n",
    "    print(f\"Final epsilon: {agent.epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c91ac5",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e459fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
