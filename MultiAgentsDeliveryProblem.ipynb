{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdbf7334",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56f910cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21105c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid world environment\n",
    "class GridWorldEnvironment:\n",
    "    def __init__(self, size=5, agents_num=4):\n",
    "        self.size = size\n",
    "        self.agents_num = agents_num\n",
    "        self.agents_positions = {}  # agent position\n",
    "        self.agents_reached_A = {}  # if agents get item\n",
    "        self.A_position = None\n",
    "        self.B_position = (size - 1, size - 1)  # fixed location of B\n",
    "        self.directions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.total_collisions = 0\n",
    "        self.total_steps = 0\n",
    "        self.agents_idx = list(range(agents_num))\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to its initial state.\n",
    "        \"\"\"\n",
    "        # initialize A position\n",
    "        self.A_position = (\n",
    "            npr.randint(0, self.size - 1),\n",
    "            npr.randint(0, self.size - 1),\n",
    "        )\n",
    "        # ensure A and B are not in the same position\n",
    "        while self.A_position == self.B_position:\n",
    "            self.A_position = (\n",
    "                npr.randint(0, self.size - 1),\n",
    "                npr.randint(0, self.size - 1),\n",
    "            )\n",
    "\n",
    "        # initialize agents' positions and reached_A status\n",
    "        self.agents_positions = {}\n",
    "        self.agents_reached_A = {}\n",
    "        for idx in self.agents_idx:\n",
    "            if npr.rand() < 0.5:\n",
    "                self.agents_positions[idx] = self.A_position\n",
    "                self.agents_reached_A[idx] = True\n",
    "            else:\n",
    "                self.agents_positions[idx] = self.B_position\n",
    "                self.agents_reached_A[idx] = False\n",
    "\n",
    "        self.total_collisions = 0\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def _get_destination(self, agent_idx):\n",
    "        \"\"\"\n",
    "        Get the destination position(A or B)\n",
    "        \"\"\"\n",
    "        return \"B\" if self.agents_reached_A[agent_idx] else \"A\"\n",
    "\n",
    "    def _find_nearby_collision_agents(self, agent_id):\n",
    "        \"\"\"\n",
    "        Find nearby agents that might collide.\n",
    "        \"\"\"\n",
    "        y, x = self.agents_positions[agent_id]\n",
    "        destination_cur = self._get_destination(agent_id)\n",
    "        nearby_agents = [\n",
    "            (-1, -1),\n",
    "            (-1, 0),\n",
    "            (-1, 1),\n",
    "            (0, -1),\n",
    "            (0, 1),\n",
    "            (1, -1),\n",
    "            (1, 0),\n",
    "            (1, 1),\n",
    "        ]\n",
    "        collision_status = []\n",
    "        for dy, dx in nearby_agents:\n",
    "            new_y, new_x = y + dy, x + dx\n",
    "            # Check if new position is valid\n",
    "            if 0 <= new_y < self.size and 0 <= new_x < self.size:\n",
    "                has_agent = 0\n",
    "                for other_agent_id in self.agents_idx:\n",
    "                    if (\n",
    "                        other_agent_id != agent_id\n",
    "                        and self.agents_positions[other_agent_id] == (new_y, new_x)\n",
    "                        and self._get_destination(other_agent_id) == destination_cur\n",
    "                    ):  # agents are going to the same destination would cause collision\n",
    "                        has_agent = 1\n",
    "                collision_status.append(has_agent)\n",
    "            else:\n",
    "                collision_status.append(0)\n",
    "        return collision_status\n",
    "\n",
    "    def get_state(self, agent_idx):\n",
    "        \"\"\"\n",
    "        Get the state of the environment for a specific agent.\n",
    "        \"\"\"\n",
    "        position = self.agents_positions[agent_idx]\n",
    "        reached_A = self.agents_reached_A[agent_idx]\n",
    "        manhattan_distance_to_A = (\n",
    "            (self.A_position[0] - position[0]),\n",
    "            (self.A_position[1] - position[1]),\n",
    "        )\n",
    "        manhattan_distance_to_B = (\n",
    "            (self.B_position[0] - position[0]),\n",
    "            (self.B_position[1] - position[1]),\n",
    "        )\n",
    "        collision_agents = self._find_nearby_collision_agents(agent_idx)\n",
    "\n",
    "        return np.array(\n",
    "            [\n",
    "                position[0],\n",
    "                position[1],\n",
    "                self.A_position[0],\n",
    "                self.A_position[1],\n",
    "                self.B_position[0],\n",
    "                self.B_position[1],\n",
    "                manhattan_distance_to_A[0],\n",
    "                manhattan_distance_to_A[1],\n",
    "                manhattan_distance_to_B[0],\n",
    "                manhattan_distance_to_B[1],\n",
    "                reached_A,\n",
    "                *collision_agents,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _check_done(self, agent_idx):\n",
    "        \"\"\"\n",
    "        Check if the agent has reached its destination.\n",
    "        \"\"\"\n",
    "        if (\n",
    "            self.agents_positions[agent_idx] == self.B_position\n",
    "            and self.agents_reached_A[agent_idx]\n",
    "        ):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def take_action(self, action_dict):\n",
    "        \"\"\"\n",
    "        Take an action in the environment and return the next state, reward and collosions.\n",
    "        \"\"\"\n",
    "        planned_actions = {}  # {action_idx: action}\n",
    "        wall_collisions = []  # number of hitting wall\n",
    "\n",
    "        for agent_idx, action in action_dict.items():\n",
    "            y, x = self.agents_positions[agent_idx]\n",
    "            if self.directions[agent_idx] == \"up\":\n",
    "                new_y, new_x = y - 1, x\n",
    "            elif self.directions[agent_idx] == \"down\":\n",
    "                new_y, new_x = y + 1, x\n",
    "            elif self.directions[agent_idx] == \"left\":\n",
    "                new_y, new_x = y, x - 1\n",
    "            elif self.directions[agent_idx] == \"right\":\n",
    "                new_y, new_x = y, x + 1\n",
    "\n",
    "            # check valid\n",
    "            if 0 <= new_y < self.size and 0 <= new_x < self.size:\n",
    "                planned_actions[agent_idx] = (new_y, new_x)  # move\n",
    "                wall_collisions.append(False)\n",
    "            else:\n",
    "                planned_actions[agent_idx] = (y, x)  # not move\n",
    "                wall_collisions.append(True)\n",
    "\n",
    "        # check collision\n",
    "        next_positions = copy.deepcopy(self.agents_positions)\n",
    "        for idx in self.agents_idx:\n",
    "            next_positions[idx] = planned_actions[idx]\n",
    "\n",
    "        collisions = 0  # number of head-on collisions\n",
    "        position_to_agents = {}\n",
    "        for agent_idx, pos in next_positions.items():\n",
    "            if pos not in position_to_agents:\n",
    "                position_to_agents[pos] = []\n",
    "            position_to_agents[pos].append(agent_idx)\n",
    "\n",
    "        agents_collisions = set()\n",
    "        for pos, agents_cur in position_to_agents.items():\n",
    "            if len(agents_cur) > 1:\n",
    "                dirs = [self._get_destination(agent_idx) for agent_idx in agents_cur]\n",
    "                if \"B\" in dirs and \"A\" in dirs:\n",
    "                    collisions += 1\n",
    "                    agents_collisions.update(agents_cur)\n",
    "\n",
    "        # update agents' positions\n",
    "        self.agents_positions = next_positions\n",
    "\n",
    "        # calculate rewards\n",
    "        rewards = {}\n",
    "        for agent_idx in self.agents_idx:\n",
    "            reward = 0\n",
    "            if wall_collisions[agent_idx]:\n",
    "                reward -= 5  # hitting wall penalty\n",
    "            else:\n",
    "                reward -= 1  # step cost\n",
    "\n",
    "            location = self.agents_positions[agent_idx]\n",
    "            if self.agents_reached_A[agent_idx]:\n",
    "                if location == self.B_position:\n",
    "                    reward += 100  # delivery success\n",
    "                    self.agents_reached_A[agent_idx] = False\n",
    "            else:\n",
    "                if location == self.A_position:\n",
    "                    reward += 50  # pickup success\n",
    "                    self.agents_reached_A[agent_idx] = True\n",
    "\n",
    "            rewards[agent_idx] = reward  # store reward\n",
    "\n",
    "        # accumulate total collisions and steps\n",
    "        self.total_collisions += collisions\n",
    "        self.total_steps += 1\n",
    "\n",
    "        # format next state\n",
    "        next_states = {}\n",
    "        for agent_idx in self.agents_idx:\n",
    "            next_states[agent_idx] = self.get_state(agent_idx)\n",
    "\n",
    "        return next_states, rewards, collisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9484e136",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d4882c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep q-learning agent\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        statespace_size,\n",
    "        action_size,\n",
    "        gamma=0.9,\n",
    "        epsilon=1,\n",
    "        epsilon_decay=0.9995,\n",
    "        min_epsilon=0.1,\n",
    "        batch_size=200,\n",
    "        replay_buffer_size=1000,\n",
    "        lr=0.001,\n",
    "        copy_frequency=100,\n",
    "    ):\n",
    "        self.statespace_size = statespace_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.lr = lr\n",
    "        self.copy_frequency = copy_frequency\n",
    "\n",
    "        self.steps = 0  # count agent's steps\n",
    "        self.replay_buffer = []  # memory\n",
    "        self.replay_buffer_size = replay_buffer_size  # memory size\n",
    "\n",
    "        # initialize the DQN\n",
    "        self.model, self.model2, self.optimizer, self.loss_fn = self.prepare_torch()\n",
    "\n",
    "    def prepare_torch(self):\n",
    "        l1 = self.statespace_size\n",
    "        l2 = 24\n",
    "        l3 = 24\n",
    "        l4 = 4\n",
    "        model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(l1, l2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(l2, l3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(l3, l4),\n",
    "        )\n",
    "        model2 = copy.deepcopy(model)\n",
    "        model2.load_state_dict(model.state_dict())\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)\n",
    "        return model, model2, optimizer, loss_fn\n",
    "\n",
    "    def update_target(self):\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        state_tensor = torch.from_numpy(state).float()\n",
    "        qvals_torch = self.model(state_tensor)\n",
    "        qvals = qvals_torch.detach().numpy()\n",
    "        return qvals\n",
    "\n",
    "    def get_maxQ(self, s):\n",
    "        return torch.max(self.model2(torch.from_numpy(s).float())).detach().numpy()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if npr.uniform() < self.epsilon:\n",
    "            action = npr.choice(self.action_size)\n",
    "        else:\n",
    "            qvals = self.get_qvals(state)\n",
    "            action = np.argmax(qvals)\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Store the transition in the replay buffer.\n",
    "        \"\"\"\n",
    "        if len(self.replay_buffer) >= self.replay_buffer_size:\n",
    "            self.replay_buffer.pop(0)\n",
    "        self.replay_buffer.append((state, action, reward, next_state))\n",
    "\n",
    "    def train_one_step(self, states, actions, targets):\n",
    "        # convert the states and actions and targets to tensors\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        targets = np.array(targets)\n",
    "\n",
    "        state_batch = torch.tensor(states, dtype=torch.float32)\n",
    "        action_batch = torch.tensor(actions, dtype=torch.long)\n",
    "        target_batch = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "        # get Q-values for the current states\n",
    "        q_values = self.model(state_batch)\n",
    "        predicted_q_values = q_values.gather(1, action_batch.unsqueeze(1)).squeeze()\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = self.loss_fn(predicted_q_values, target_batch)\n",
    "\n",
    "        # backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the agent using the replay buffer.\n",
    "        \"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return  # samples not enough\n",
    "\n",
    "        # sample a batch from the replay buffer\n",
    "        minibatch = random.sample(\n",
    "            self.replay_buffer,\n",
    "            self.batch_size,\n",
    "        )\n",
    "        states, actions, rewards, next_states = zip(*minibatch)\n",
    "\n",
    "        # TD targets\n",
    "        targets = []\n",
    "        for i in range(len(minibatch)):\n",
    "            next_maxQ = self.get_maxQ(next_states[i])\n",
    "            action_target = rewards[i] + self.gamma * next_maxQ\n",
    "            targets.append(action_target)\n",
    "\n",
    "        # train the model\n",
    "        loss = self.train_one_step(states, actions, targets)\n",
    "\n",
    "        # update network periodically\n",
    "        self.steps += 1\n",
    "        if self.steps % self.copy_frequency == 0:\n",
    "            self.update_target()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # decay epsilon\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b27cf79",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41721ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "def train_agents(\n",
    "    agent, env, max_steps=1500000, max_collisions=4000, max_walltime=60, verbose=True\n",
    "):\n",
    "    \"\"\"Training each agent in the environment.\"\"\"\n",
    "    # start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # initialize the environment\n",
    "    env._reset()\n",
    "\n",
    "    # initial states\n",
    "    states = {agent_idx: env.get_state(agent_idx) for agent_idx in env.agents_idx}\n",
    "\n",
    "    # global variables\n",
    "    total_collisions = 0\n",
    "    total_steps = 0\n",
    "    episode = 0\n",
    "\n",
    "    while total_collisions <= max_collisions and total_steps <= max_steps:\n",
    "        if time.time() - start_time > max_walltime:\n",
    "            print(\"===== Time limit exceeded. =====\")\n",
    "            break\n",
    "\n",
    "        actions_dict = {}\n",
    "        for agent_idx in env.agents_idx:  # central clock - fix order\n",
    "            action = agent.get_action(states[agent_idx])\n",
    "            actions_dict[agent_idx] = action\n",
    "\n",
    "        # take action in the environment\n",
    "        next_states, rewards, collisions = env.take_action(actions_dict)\n",
    "        # store transition in replay buffer\n",
    "        for agent_idx in env.agents_idx:\n",
    "            state = states[agent_idx]\n",
    "            action = actions_dict[agent_idx]\n",
    "            reward = rewards[agent_idx]\n",
    "            next_state = next_states[agent_idx]\n",
    "            agent.store_transition(state, action, reward, next_state)\n",
    "\n",
    "        # train the agent\n",
    "        if len(agent.replay_buffer) >= agent.batch_size:\n",
    "            loss = agent.train()\n",
    "\n",
    "        # update the states\n",
    "        states = next_states\n",
    "\n",
    "        total_collisions += collisions\n",
    "        total_steps += 1\n",
    "        episode += 1\n",
    "\n",
    "        agent.decay_epsilon()\n",
    "        if verbose and total_steps % 10000 == len(env.agents_idx):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(\n",
    "                f\"Steps: {total_steps}/{max_steps}, Collisions: {total_collisions}/{max_collisions}, Epsilon: {agent.epsilon:.3f}, Time Elapsed: {elapsed_time:.2f}s\"\n",
    "            )\n",
    "    \n",
    "    print(\"Training completed.\")\n",
    "    print(f\"Total steps: {total_steps}\")\n",
    "    print(f\"Total collisions: {total_collisions}\")\n",
    "    print(f\"Final epsilon: {agent.epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c91ac5",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02e459fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agents(agent, env, test_times=100, max_steps=25, step_verbose=True):\n",
    "    \"\"\"\n",
    "    Test the trained agent in the environment.\n",
    "    \"\"\"\n",
    "    # initialize the parmeters\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0\n",
    "    success_times = 0\n",
    "    total_steps_successful = 0\n",
    "    total_collisions = 0\n",
    "\n",
    "    for run in range(test_times):\n",
    "        # reset the environment\n",
    "        env._reset()\n",
    "\n",
    "        # all agents start at B(as the assingment requirenment says)\n",
    "        for agent_idx in env.agents_idx:\n",
    "            env.agents_positions[agent_idx] = env.B_position\n",
    "            env.agents_reached_A[agent_idx] = False\n",
    "\n",
    "        # initial states\n",
    "        states = {agent_idx: env.get_state(agent_idx) for agent_idx in env.agents_idx}\n",
    "\n",
    "        # initialize the variables\n",
    "        delivery_success = {agent_idx: False for agent_idx in env.agents_idx}\n",
    "        steps_taken = {agent_idx: 0 for agent_idx in env.agents_idx}\n",
    "        collisions_happened = False\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # don't need randomize the order of agents as my options\n",
    "            actions_dict = {}\n",
    "            for agent_idx in env.agents_idx:\n",
    "                action = agent.get_action(states[agent_idx])\n",
    "                actions_dict[agent_idx] = action\n",
    "\n",
    "            # take action in the environment\n",
    "            next_states, rewards, collisions = env.take_action(actions_dict)\n",
    "            # update the states\n",
    "            states = next_states\n",
    "            # update stepts taken\n",
    "            for agent_idx in env.agents_idx:\n",
    "                steps_taken[agent_idx] += 1\n",
    "\n",
    "            # check collisions\n",
    "            if collisions > 0:\n",
    "                collisions_happened = True\n",
    "\n",
    "            # checl delivery success\n",
    "            for agent_idx in env.agents_idx:\n",
    "                position = env.agents_positions[agent_idx]\n",
    "                reached_A = env.agents_reached_A[agent_idx]\n",
    "\n",
    "                if (\n",
    "                    position == env.B_position\n",
    "                    and not reached_A\n",
    "                    and steps_taken[agent_idx] > 0\n",
    "                ):\n",
    "                    if not delivery_success[agent_idx]:\n",
    "                        delivery_success[agent_idx] = True\n",
    "\n",
    "            # check if all agents have successfully delivered\n",
    "            if all(delivery_success.values()):\n",
    "                break\n",
    "\n",
    "        # record successful deliveries\n",
    "        for agent_idx in env.agents_idx:\n",
    "            if (\n",
    "                delivery_success[agent_idx]\n",
    "                and not collisions_happened\n",
    "                and steps_taken[agent_idx] <= max_steps\n",
    "            ):\n",
    "                success_times += 1\n",
    "                total_steps_successful += steps_taken[agent_idx]\n",
    "\n",
    "        # record collisions\n",
    "        if collisions_happened:\n",
    "            total_collisions += 1\n",
    "        # print step verbose\n",
    "        if step_verbose:\n",
    "            print(\n",
    "                f\"[TEST] Run {run + 1}/{test_times}: Success deliveries = {sum(delivery_success.values())}, Collisions: {collisions_happened}\"\n",
    "            )\n",
    "    \n",
    "    # restore the epsilon\n",
    "    agent.epsilon = original_epsilon\n",
    "\n",
    "    # test indicators\n",
    "    total_possible_deliveries = len(env.agents_idx) * test_times\n",
    "    success_rate = (success_times / total_possible_deliveries) * 100\n",
    "    avg_steps = total_steps_successful / success_times if success_times > 0 else 0\n",
    "    collisions_rate = (total_collisions / test_times) * 100\n",
    "\n",
    "    # test summary\n",
    "    print('\\n ===== Test Summary =====')\n",
    "    print(f\"Success rate: {success_rate:.2f}%\")\n",
    "    print(f\"Average steps per successful delivery: {avg_steps:.2f}\")\n",
    "    print(f\"Collision Rate: {collisions_rate:.2f}%\")\n",
    "\n",
    "    return {\n",
    "        \"success_rate\": success_rate,\n",
    "        \"avg_steps\": avg_steps,\n",
    "        \"collisions_rate\": collisions_rate,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522faa76",
   "metadata": {},
   "source": [
    "## Test process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7b8332a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training with steps: 500000 =====\n",
      "===== Time limit exceeded. =====\n",
      "Training completed.\n",
      "Total steps: 7643\n",
      "Total collisions: 0\n",
      "Final epsilon: 0.100\n",
      "===== Time limit exceeded. =====\n",
      "Training completed.\n",
      "Total steps: 7643\n",
      "Total collisions: 0\n",
      "Final epsilon: 0.100\n",
      "\n",
      " ===== Test Summary =====\n",
      "Success rate: 50.00%\n",
      "Average steps per successful delivery: 25.00\n",
      "Collision Rate: 0.00%\n",
      "\n",
      "===== Training with steps: 1000000 =====\n",
      "\n",
      " ===== Test Summary =====\n",
      "Success rate: 50.00%\n",
      "Average steps per successful delivery: 25.00\n",
      "Collision Rate: 0.00%\n",
      "\n",
      "===== Training with steps: 1000000 =====\n",
      "Training completed.\n",
      "Total steps: 4004\n",
      "Total collisions: 4001\n",
      "Final epsilon: 0.135\n",
      "Training completed.\n",
      "Total steps: 4004\n",
      "Total collisions: 4001\n",
      "Final epsilon: 0.135\n",
      "\n",
      " ===== Test Summary =====\n",
      "Success rate: 50.00%\n",
      "Average steps per successful delivery: 25.00\n",
      "Collision Rate: 0.00%\n",
      "\n",
      "===== Training with steps: 1500000 =====\n",
      "\n",
      " ===== Test Summary =====\n",
      "Success rate: 50.00%\n",
      "Average steps per successful delivery: 25.00\n",
      "Collision Rate: 0.00%\n",
      "\n",
      "===== Training with steps: 1500000 =====\n",
      "===== Time limit exceeded. =====\n",
      "Training completed.\n",
      "Total steps: 7481\n",
      "Total collisions: 0\n",
      "Final epsilon: 0.100\n",
      "===== Time limit exceeded. =====\n",
      "Training completed.\n",
      "Total steps: 7481\n",
      "Total collisions: 0\n",
      "Final epsilon: 0.100\n",
      "\n",
      " ===== Test Summary =====\n",
      "Success rate: 50.00%\n",
      "Average steps per successful delivery: 25.00\n",
      "Collision Rate: 0.00%\n",
      "\n",
      "===== Generalization Test =====\n",
      "Steps: 4/1500000, Collisions: 0/4000, Epsilon: 0.998, Time Elapsed: 0.00s\n",
      "\n",
      " ===== Test Summary =====\n",
      "Success rate: 50.00%\n",
      "Average steps per successful delivery: 25.00\n",
      "Collision Rate: 0.00%\n",
      "\n",
      "===== Generalization Test =====\n",
      "Steps: 4/1500000, Collisions: 0/4000, Epsilon: 0.998, Time Elapsed: 0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/usr/local/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/usr/local/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Time limit exceeded. =====\n",
      "Training completed.\n",
      "Total steps: 7501\n",
      "Total collisions: 0\n",
      "Final epsilon: 0.100\n",
      "\n",
      " ===== Test Summary =====\n",
      "Success rate: 50.00%\n",
      "Average steps per successful delivery: 25.00\n",
      "Collision Rate: 0.00%\n",
      "\n",
      "===== Test 2 Final test metrics =====\n",
      "Success rate: 50.00%\n",
      "Average steps: 25.00%\n",
      "Collision rate: 0.00%\n",
      "\n",
      " ===== Test Summary =====\n",
      "Success rate: 50.00%\n",
      "Average steps per successful delivery: 25.00\n",
      "Collision Rate: 0.00%\n",
      "\n",
      "===== Test 2 Final test metrics =====\n",
      "Success rate: 50.00%\n",
      "Average steps: 25.00%\n",
      "Collision rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# setup the environment\n",
    "test_env = GridWorldEnvironment()\n",
    "test_state = test_env.get_state(0)\n",
    "statespace_size = test_state.shape[0]\n",
    "action_size = len(test_env.directions)\n",
    "\n",
    "# trainning parameters\n",
    "trainning_steps = [500000, 1000000, 1500000]\n",
    "test_runs = 100\n",
    "repeat_times = 2\n",
    "\n",
    "# results metrics\n",
    "results = {\n",
    "    \"training_steps\": [],\n",
    "    \"success_rate_mean\": [],\n",
    "    \"success_rate_std\": [],\n",
    "    \"average_steps_mean\": [],\n",
    "    \"average_steps_std\": [],\n",
    "    \"collision_rate_mean\": [],\n",
    "    \"collision_rate_std\": [],\n",
    "}\n",
    "\n",
    "# Test 1: trainning steps & performance\n",
    "for steps in trainning_steps:\n",
    "    print(\"\\n===== Training with steps: {} =====\".format(steps))\n",
    "\n",
    "    success_rates = []\n",
    "    average_steps_list = []\n",
    "    collision_rates = []\n",
    "\n",
    "    # initialize the agent and environment\n",
    "    env = GridWorldEnvironment()\n",
    "    agent = Agent(\n",
    "        statespace_size,\n",
    "        action_size,\n",
    "        gamma=0.9,\n",
    "        epsilon=1,\n",
    "        epsilon_decay=0.9995,\n",
    "        min_epsilon=0.1,\n",
    "        batch_size=200,\n",
    "        replay_buffer_size=5000,\n",
    "        lr=0.001,\n",
    "        copy_frequency=500,\n",
    "    )\n",
    "\n",
    "    # train the agent\n",
    "    train_agents(\n",
    "        agent,\n",
    "        env,\n",
    "        max_steps=steps,\n",
    "        max_collisions=4000,\n",
    "        # max_walltime=600,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # test the agent\n",
    "    metrics = test_agents(\n",
    "        agent, env, test_times=test_runs, max_steps=25, step_verbose=False\n",
    "    )\n",
    "\n",
    "    # record the results\n",
    "    success_rate = metrics[\"success_rate\"]\n",
    "    average_steps_list = metrics[\"avg_steps\"]\n",
    "    collision_rates = metrics[\"collisions_rate\"]\n",
    "\n",
    "# calculate and store the results\n",
    "results[\"training_steps\"].append(steps)\n",
    "results[\"success_rate_mean\"].append(np.mean(success_rates))\n",
    "results[\"success_rate_std\"].append(np.std(success_rates))\n",
    "results[\"average_steps_mean\"].append(np.mean(average_steps_list))\n",
    "results[\"average_steps_std\"].append(np.std(average_steps_list))\n",
    "results[\"collision_rate_mean\"].append(np.mean(collision_rates))\n",
    "results[\"collision_rate_std\"].append(np.std(collision_rates))\n",
    "\n",
    "\n",
    "# Test 2: Generalization\n",
    "print(\"\\n===== Generalization Test =====\")\n",
    "env = GridWorldEnvironment()\n",
    "agent = Agent(\n",
    "    statespace_size,\n",
    "    action_size,\n",
    "    gamma=0.9,\n",
    "    epsilon=1,\n",
    "    epsilon_decay=0.9995,\n",
    "    min_epsilon=0.1,\n",
    "    batch_size=200,\n",
    "    replay_buffer_size=5000,\n",
    "    lr=0.001,\n",
    "    copy_frequency=500,\n",
    ")\n",
    "train_agents(\n",
    "    agent,\n",
    "    env,\n",
    "    max_steps=1500000,\n",
    "    max_collisions=4000,\n",
    "    # max_walltime=600,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "metrics = test_agents(\n",
    "    agent, env, test_times=200, max_steps=25, step_verbose=False\n",
    ")\n",
    "\n",
    "print(\"\\n===== Test 2 Final test metrics =====\")\n",
    "print(f\"Success rate: {metrics['success_rate']:.2f}%\")\n",
    "print(f\"Average steps: {metrics['avg_steps']:.2f}%\")\n",
    "print(f\"Collision rate: {metrics['collisions_rate']:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
