{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56f910cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import numpy.random as npr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21105c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid world environment\n",
    "class GridWorldEnvironment:\n",
    "    def __init__(self, size=5, agents_num=4):\n",
    "        self.size = size\n",
    "        self.agents_num = agents_num\n",
    "        self.agents_positions = {}  # agent position\n",
    "        self.agents_reached_A = {}  # if agents get item\n",
    "        self.A_position = None\n",
    "        self.B_position = (size - 1, size - 1)  # fixed location of B\n",
    "        self.directions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.total_collisions = 0\n",
    "        self.total_steps = 0\n",
    "        self.agents_idx = list(range(agents_num))\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to its initial state.\n",
    "        \"\"\"\n",
    "        # initialize A position\n",
    "        self.A_position = (\n",
    "            npr.randint(0, self.size - 1),\n",
    "            npr.randint(0, self.size - 1),\n",
    "        )\n",
    "        # ensure A and B are not in the same position\n",
    "        while self.A_position == self.B_position:\n",
    "            self.A_position = (\n",
    "                npr.randint(0, self.size - 1),\n",
    "                npr.randint(0, self.size - 1),\n",
    "            )\n",
    "\n",
    "        # initialize agents' positions and reached_A status\n",
    "        self.agents_positions = {}\n",
    "        self.agents_reached_A = {}\n",
    "        for idx in self.agents_idx:\n",
    "            if npr.rand() < 0.5:\n",
    "                self.agents_positions[idx] = self.A_position\n",
    "                self.agents_reached_A[idx] = True\n",
    "            else:\n",
    "                self.agents_positions[idx] = self.B_position\n",
    "                self.agents_reached_A[idx] = False\n",
    "\n",
    "        self.total_collisions = 0\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def _get_destination(self, agent_idx):\n",
    "        \"\"\"\n",
    "        Get the destination position(A or B)\n",
    "        \"\"\"\n",
    "        return \"B\" if self.agents_reached_A[agent_idx] else \"A\"\n",
    "\n",
    "    def _find_nearby_collision_agents(self, agent_id):\n",
    "        \"\"\"\n",
    "        Find nearby agents that might collide.\n",
    "        \"\"\"\n",
    "        y, x = self.agents_positions[agent_id]\n",
    "        destination_cur = self._get_destination(agent_id)\n",
    "        nearby_agents = [\n",
    "            (-1, -1),\n",
    "            (-1, 0),\n",
    "            (-1, 1),\n",
    "            (0, -1),\n",
    "            (0, 1),\n",
    "            (1, -1),\n",
    "            (1, 0),\n",
    "            (1, 1),\n",
    "        ]\n",
    "        collision_status = []\n",
    "        for dy, dx in nearby_agents:\n",
    "            new_y, new_x = y + dy, x + dx\n",
    "            # Check if new position is valid\n",
    "            if 0 <= new_y < self.size and 0 <= new_x < self.size:\n",
    "                collision = False\n",
    "                for other_agent_id in self.agents_idx:\n",
    "                    if (\n",
    "                        other_agent_id != agent_id\n",
    "                        and self.agents_positions[other_agent_id] == (new_y, new_x)\n",
    "                        and self._get_destination(other_agent_id) == destination_cur\n",
    "                    ):  # agents are going to the same destination would cause collision\n",
    "                        collision = 1\n",
    "                collision_status.append(collision)\n",
    "            else:\n",
    "                collision_status.append(0)\n",
    "\n",
    "    def get_state(self, agent_idx):\n",
    "        \"\"\"\n",
    "        Get the state of the environment for a specific agent.\n",
    "        \"\"\"\n",
    "        position = self.agents_positions[agent_idx]\n",
    "        reached_A = self.agents_reached_A[agent_idx]\n",
    "        manhattan_distance_to_A = (self.A_position[0] - position[0]) + (\n",
    "            self.A_position[1] - position[1]\n",
    "        )\n",
    "        manhattan_distance_to_B = (self.B_position[0] - position[0]) + (\n",
    "            self.B_position[1] - position[1]\n",
    "        )\n",
    "        collision_agents = self._find_nearby_collision_agents(agent_idx)\n",
    "\n",
    "        return np.array(\n",
    "            [\n",
    "                position[0],\n",
    "                position[1],\n",
    "                self.A_position[0],\n",
    "                self.A_position[1],\n",
    "                self.B_position[0],\n",
    "                self.B_position[1],\n",
    "                manhattan_distance_to_A[0],\n",
    "                manhattan_distance_to_A[1],\n",
    "                manhattan_distance_to_B[0],\n",
    "                manhattan_distance_to_B[1],\n",
    "                reached_A,\n",
    "                *collision_agents,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _check_done(self, agent_idx):\n",
    "        \"\"\"\n",
    "        Check if the agent has reached its destination.\n",
    "        \"\"\"\n",
    "        if (\n",
    "            self.agents_positions[agent_idx] == self.B_position\n",
    "            and self.agents_reached_A[agent_idx]\n",
    "        ):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def take_action(self, action_dict):\n",
    "        \"\"\"\n",
    "        Take an action in the environment and return the next state, reward and collosions.\n",
    "        \"\"\"\n",
    "        planned_actions = {}  # {action_idx: action}\n",
    "        wall_collisions = []  # number of hitting wall\n",
    "\n",
    "        for agent_idx, action in action_dict.items():\n",
    "            y, x = self.agents_positions[agent_idx]\n",
    "            if self.agents_positions[agent_idx] == \"up\":\n",
    "                new_y, new_x = y - 1, x\n",
    "            elif self.agents_positions[agent_idx] == \"down\":\n",
    "                new_y, new_x = y + 1, x\n",
    "            elif self.agents_positions[agent_idx] == \"left\":\n",
    "                new_y, new_x = y, x - 1\n",
    "            elif self.agents_positions[agent_idx] == \"right\":\n",
    "                new_y, new_x = y, x + 1\n",
    "\n",
    "            # check valid\n",
    "            if 0 <= new_y < self.size and 0 <= new_x < self.size:\n",
    "                planned_actions[agent_idx] = (new_y, new_x)  # move\n",
    "                wall_collisions.append(False)\n",
    "            else:\n",
    "                planned_actions[agent_idx] = (y, x)  # not move\n",
    "                wall_collisions.append(True)\n",
    "\n",
    "        # check collision\n",
    "        next_positions = copy.deepcopy(self.agents_positions)\n",
    "        for idx in self.agents_idx:\n",
    "            next_positions[idx] = planned_actions[idx]\n",
    "\n",
    "        collisions = 0  # number of head-on collisions\n",
    "        position_to_agents = {}\n",
    "        for agent_idx, pos in next_positions.items():\n",
    "            if pos not in position_to_agents:\n",
    "                position_to_agents[pos] = []\n",
    "            position_to_agents[pos].append(agent_idx)\n",
    "\n",
    "        agents_collisions = set()\n",
    "        for pos, agents_cur in position_to_agents.items():\n",
    "            if len(agents_cur) > 1:\n",
    "                dirs = [self._get_destination(agent_idx) for agent_idx in agents_cur]\n",
    "                if \"B\" in dirs and \"A\" in dirs:\n",
    "                    collisions += 1\n",
    "                    agents_collisions.update(agents_cur)\n",
    "\n",
    "        # update agents' positions\n",
    "        self.agents_positions = next_positions\n",
    "\n",
    "        # calculate rewards\n",
    "        rewards = {}\n",
    "        for agent_idx in self.agents_idx:\n",
    "            reward = 0\n",
    "            if wall_collisions[agent_idx]:\n",
    "                reward -= 5  # hitting wall penalty\n",
    "            else:\n",
    "                reward -= 1  # step cost\n",
    "\n",
    "            location = self.agents_positions[agent_idx]\n",
    "            if self.agents_reached_A[agent_idx]:\n",
    "                if location == self.B_position:\n",
    "                    reward += 100  # delivery success\n",
    "                    self.agents_reached_A[agent_idx] = False\n",
    "            else:\n",
    "                if location == self.A_position:\n",
    "                    reward += 50  # pickup success\n",
    "                    self.agents_reached_A[agent_idx] = True\n",
    "\n",
    "        rewards[agent_idx] = reward  # store reward\n",
    "\n",
    "        # accumulate total collisions and steps\n",
    "        self.total_collisions += collisions\n",
    "        self.total_steps += 1\n",
    "\n",
    "        # format next state\n",
    "        next_states = {}\n",
    "        for agent_idx in self.agents_idx:\n",
    "            next_states[agent_idx] = self.get_state(agent_idx)\n",
    "\n",
    "        return next_states, rewards, collisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "315ca0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def movingAverage(arr, window_size):\n",
    "    i = 0\n",
    "    moving_averages = []\n",
    "    while i < len(arr) - window_size + 1:\n",
    "        window_average = round(np.sum(arr[i : i + window_size]) / window_size, 2)\n",
    "        moving_averages.append(window_average)\n",
    "        i += 1\n",
    "    return moving_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66e4acbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridWorldEnvironment' object has no attribute 'agent_position'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m number_of_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     29\u001b[0m reward_per_episode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m number_of_steps \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_steps \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m---> 31\u001b[0m     \u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_position\u001b[49m \u001b[38;5;241m!=\u001b[39m environment\u001b[38;5;241m.\u001b[39mnest_location\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m environment\u001b[38;5;241m.\u001b[39mreached_A \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     33\u001b[0m ):  \u001b[38;5;66;03m# Continue until reaching location B\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# why does this never seem to run into an infinite loop? number_of_steps wasn't tested by Yue\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Presumably the high and everlasting epsilon lets everyone walk into one of the goals sooner or later\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     state \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mget_state()\n\u001b[1;32m     37\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(state, epsilon)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridWorldEnvironment' object has no attribute 'agent_position'"
     ]
    }
   ],
   "source": [
    "from time import process_time\n",
    "\n",
    "t = process_time()\n",
    "\n",
    "# Define the learning parameters\n",
    "learning_rate = 0.3\n",
    "discount_factor = 0.98\n",
    "num_episodes = 50000\n",
    "max_steps = 100  # this was never used - 100 is good\n",
    "epsilon_initial = 1.0\n",
    "epsilon_final = 0.1\n",
    "epsilon_decay = 0.99995\n",
    "# Define the grid world dimensions\n",
    "grid_rows = 5\n",
    "grid_cols = 5\n",
    "# Define the number of actions (up, down, left, right)\n",
    "num_actions = 4\n",
    "\n",
    "# Create the Q-table agent and grid world environment\n",
    "agent = QTableAgent()\n",
    "environment = GridWorldEnvironment(grid_rows, grid_cols)\n",
    "\n",
    "reward_total = []\n",
    "epsilon = epsilon_initial\n",
    "# Implement the Q-learning algorithm\n",
    "for episode in range(num_episodes + 1):\n",
    "    environment._reset()\n",
    "    number_of_steps = 0\n",
    "    reward_per_episode = 0\n",
    "    while number_of_steps <= max_steps and (\n",
    "        environment.agent_position != environment.nest_location\n",
    "        or environment.reached_A != True\n",
    "    ):  # Continue until reaching location B\n",
    "        # why does this never seem to run into an infinite loop? number_of_steps wasn't tested by Yue\n",
    "        # Presumably the high and everlasting epsilon lets everyone walk into one of the goals sooner or later\n",
    "        state = environment.get_state()\n",
    "        action = agent.choose_action(state, epsilon)\n",
    "        reward = environment.take_action(action)\n",
    "        next_state = environment.get_state()\n",
    "        reward_per_episode += reward\n",
    "        done = environment.check_done()\n",
    "        agent.update_q_table(\n",
    "            state, action, next_state, reward, learning_rate, discount_factor, done\n",
    "        )\n",
    "        number_of_steps += 1\n",
    "    reward_total.append(reward_per_episode)\n",
    "    epsilon = np.max([epsilon_final, epsilon * epsilon_decay])\n",
    "    if episode % 5000 == 0:\n",
    "        print(episode, \"episodes\")\n",
    "\n",
    "elapsed_time = process_time() - t\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(movingAverage(reward_total, 100))\n",
    "# Add x and y axis labels\n",
    "plt.xlabel(\"#episodes\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.title(\"Q table Agent\")\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "agent.q_table.astype(\"float32\").tofile(\"qTable single agent fast.dat\")\n",
    "\n",
    "print(\"Finished in \", elapsed_time, \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2116707",
   "metadata": {},
   "source": [
    "You need to instantiate the constant for the size of the state space below. This will be used as the size of the input tensor for your Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab4bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "statespace_size = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63f7fbc",
   "metadata": {},
   "source": [
    "The function \"prepare_torch\" needs to be called once and only once at the start of your program to initialise PyTorch and generate the two Q-networks. It returns the target model (for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211c3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_torch():\n",
    "    global statespace_size\n",
    "    global model, model2\n",
    "    global optimizer\n",
    "    global loss_fn\n",
    "    l1 = statespace_size\n",
    "    l2 = 24\n",
    "    l3 = 24\n",
    "    l4 = 4\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(l1, l2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(l2, l3),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(l3, l4),\n",
    "    )\n",
    "    model2 = copy.deepcopy(model)\n",
    "    model2.load_state_dict(model.state_dict())\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    learning_rate = 1e-3\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11d77f6",
   "metadata": {},
   "source": [
    "The function \"update_target\" copies the state of the prediction network to the target network. You need to use this in regular intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target():\n",
    "    global model, model2\n",
    "    model2.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c813d2",
   "metadata": {},
   "source": [
    "The function \"get_qvals\" returns a numpy list of qvals for the state given by the argument _based on the prediction network_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb3a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qvals(state):\n",
    "    state1 = torch.from_numpy(state).float()\n",
    "    qvals_torch = model(state1)\n",
    "    qvals = qvals_torch.data.numpy()\n",
    "    return qvals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced681b9",
   "metadata": {},
   "source": [
    "The function \"get_maxQ\" returns the maximum q-value for the state given by the argument _based on the target network_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b6c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maxQ(s):\n",
    "    return torch.max(model2(torch.from_numpy(s).float())).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b27cf79",
   "metadata": {},
   "source": [
    "The function \"train_one_step_new\" performs a single training step. It returns the current loss (only needed for debugging purposes). Its parameters are three parallel lists: a minibatch of states, a minibatch of actions, a minibatch of the corresponding TD targets and the discount factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a34c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step(states, actions, targets, gamma):\n",
    "    # pass to this function: state1_batch, action_batch, TD_batch\n",
    "    global model, model2\n",
    "    state1_batch = torch.cat([torch.from_numpy(s).float() for s in states])\n",
    "    action_batch = torch.Tensor(actions)\n",
    "    Q1 = model(state1_batch)\n",
    "    X = Q1.gather(dim=1, index=action_batch.long().unsqueeze(dim=1)).squeeze()\n",
    "    Y = torch.tensor(targets)\n",
    "    loss = loss_fn(X, Y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e58e22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
