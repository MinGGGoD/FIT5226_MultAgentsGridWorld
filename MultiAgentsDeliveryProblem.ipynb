{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdbf7334",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "942cede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "56f910cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21105c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid world environment\n",
    "class GridWorldEnvironment:\n",
    "    def __init__(self, size=5, agents_num=4):\n",
    "        self.size = size\n",
    "        self.agents_num = agents_num\n",
    "        self.agents_positions = {}  # agent position\n",
    "        self.agents_reached_A = {}  # if agents get item\n",
    "        self.A_position = None\n",
    "        self.B_position = (size - 1, size - 1)  # fixed location of B\n",
    "        self.directions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.total_collisions = 0\n",
    "        self.total_steps = 0\n",
    "        self.agents_idx = list(range(agents_num))\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to its initial state.\n",
    "        \"\"\"\n",
    "        # initialize A position\n",
    "        self.A_position = (\n",
    "            npr.randint(0, self.size - 1),\n",
    "            npr.randint(0, self.size - 1),\n",
    "        )\n",
    "        # ensure A and B are not in the same position\n",
    "        while self.A_position == self.B_position:\n",
    "            self.A_position = (\n",
    "                npr.randint(0, self.size - 1),\n",
    "                npr.randint(0, self.size - 1),\n",
    "            )\n",
    "\n",
    "        # initialize agents' positions and reached_A status\n",
    "        self.agents_positions = {}\n",
    "        self.agents_reached_A = {}\n",
    "        for idx in self.agents_idx:\n",
    "            if npr.rand() > 0.5:\n",
    "                self.agents_positions[idx] = self.A_position\n",
    "                self.agents_reached_A[idx] = True\n",
    "            else:\n",
    "                self.agents_positions[idx] = self.B_position\n",
    "                self.agents_reached_A[idx] = False\n",
    "\n",
    "        self.total_collisions = 0\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def _get_destination(self, agent_idx):\n",
    "        \"\"\"\n",
    "        Get the destination position(A or B)\n",
    "        \"\"\"\n",
    "        return \"B\" if self.agents_reached_A[agent_idx] else \"A\"\n",
    "\n",
    "    def _find_nearby_collision_agents(self, agent_id):\n",
    "        \"\"\"\n",
    "        Find nearby agents that might collide.\n",
    "        \"\"\"\n",
    "        y, x = self.agents_positions[agent_id]\n",
    "        destination_cur = self._get_destination(agent_id)\n",
    "        nearby_agents = [\n",
    "            (-1, -1),\n",
    "            (-1, 0),\n",
    "            (-1, 1),\n",
    "            (0, -1),\n",
    "            (0, 1),\n",
    "            (1, -1),\n",
    "            (1, 0),\n",
    "            (1, 1),\n",
    "        ]\n",
    "        collision_status = []\n",
    "        for dy, dx in nearby_agents:\n",
    "            new_y, new_x = y + dy, x + dx\n",
    "            # Check if new position is valid\n",
    "            if 0 <= new_y < self.size and 0 <= new_x < self.size:\n",
    "                has_agent = 0\n",
    "                for other_agent_id in self.agents_idx:\n",
    "                    if (\n",
    "                        other_agent_id != agent_id\n",
    "                        and self.agents_positions[other_agent_id] == (new_y, new_x)\n",
    "                        and self._get_destination(other_agent_id) != destination_cur\n",
    "                    ):  # agents are going to the same destination would cause collision\n",
    "                        has_agent = 1\n",
    "                collision_status.append(has_agent)\n",
    "            else:\n",
    "                collision_status.append(0)\n",
    "        return collision_status\n",
    "\n",
    "    def get_state(self, agent_idx):\n",
    "        \"\"\"\n",
    "        Get the state of the environment for a specific agent.\n",
    "        \"\"\"\n",
    "        position = self.agents_positions[agent_idx]\n",
    "        reached_A = self.agents_reached_A[agent_idx]\n",
    "        # manhattan_distance_to_A = (\n",
    "        #     (self.A_position[0] - position[0]),\n",
    "        #     (self.A_position[1] - position[1]),\n",
    "        # )\n",
    "        collision_agents = self._find_nearby_collision_agents(agent_idx)\n",
    "\n",
    "        return np.array(\n",
    "            [\n",
    "                position[0],\n",
    "                position[1],\n",
    "                self.A_position[0],\n",
    "                self.A_position[1],\n",
    "                # manhattan_distance_to_A[0],\n",
    "                # manhattan_distance_to_A[1],\n",
    "                reached_A,\n",
    "                *collision_agents,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _check_done(self, agent_idx):\n",
    "        \"\"\"\n",
    "        Check if the agent has reached its destination.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.agents_positions[agent_idx] == self.B_position\n",
    "            and self.agents_reached_A[agent_idx]\n",
    "        )\n",
    "\n",
    "    def take_action(self, action_dict):\n",
    "        \"\"\"\n",
    "        Take an action in the environment and return the next state, reward and collosions.\n",
    "        \"\"\"\n",
    "        planned_actions = {}  # {action_idx: action}\n",
    "        wall_collisions = []  # number of hitting wall\n",
    "\n",
    "        for agent_idx, action in action_dict.items():\n",
    "            y, x = self.agents_positions[agent_idx]\n",
    "            if self.directions[action] == \"up\":\n",
    "                new_y, new_x = y - 1, x\n",
    "            elif self.directions[action] == \"down\":\n",
    "                new_y, new_x = y + 1, x\n",
    "            elif self.directions[action] == \"left\":\n",
    "                new_y, new_x = y, x - 1\n",
    "            elif self.directions[action] == \"right\":\n",
    "                new_y, new_x = y, x + 1\n",
    "\n",
    "            # check valid\n",
    "            if 0 <= new_y < self.size and 0 <= new_x < self.size:\n",
    "                planned_actions[agent_idx] = (new_y, new_x)  # move\n",
    "                wall_collisions.append(False)\n",
    "            else:\n",
    "                planned_actions[agent_idx] = (y, x)  # not move\n",
    "                wall_collisions.append(True)\n",
    "\n",
    "        # check collision\n",
    "        next_positions = copy.deepcopy(self.agents_positions)\n",
    "        for idx in self.agents_idx:\n",
    "            next_positions[idx] = planned_actions[idx]\n",
    "\n",
    "        collisions = 0  # number of head-on collisions\n",
    "        position_to_agents = {}\n",
    "        for agent_idx, pos in next_positions.items():\n",
    "            if pos not in position_to_agents:\n",
    "                position_to_agents[pos] = []\n",
    "            position_to_agents[pos].append(agent_idx)\n",
    "        agents_collisions = set()\n",
    "        for pos, agents_cur in position_to_agents.items():\n",
    "            if pos == self.A_position or pos == self.B_position:\n",
    "                continue  # ignore overlap at A or B\n",
    "            if len(agents_cur) > 1:\n",
    "                dirs = [self._get_destination(a) for a in agents_cur]\n",
    "                if \"B\" in dirs and \"A\" in dirs:\n",
    "                    collisions += 1\n",
    "                    agents_collisions.update(agents_cur)\n",
    "\n",
    "        # check swamp collision\n",
    "        for i in self.agents_idx:\n",
    "            for j in self.agents_idx:\n",
    "                if i < j:  # 避免重复检查\n",
    "                    current_pos_i = self.agents_positions[i]\n",
    "                    next_pos_i = planned_actions[i]\n",
    "                    current_pos_j = self.agents_positions[j]\n",
    "                    next_pos_j = planned_actions[j]\n",
    "                    # 判断是否为对向穿越\n",
    "                    if (\n",
    "                        next_pos_i == current_pos_j\n",
    "                        and next_pos_j == current_pos_i\n",
    "                        and self._get_destination(i) != self._get_destination(j)\n",
    "                        and current_pos_i != self.A_position\n",
    "                        and current_pos_i != self.B_position\n",
    "                        and current_pos_j != self.A_position\n",
    "                        and current_pos_j != self.B_position\n",
    "                    ):\n",
    "                        collisions += 1\n",
    "                        agents_collisions.add(i)\n",
    "                        agents_collisions.add(j)\n",
    "\n",
    "        # calculate rewards\n",
    "        rewards = {}\n",
    "        for agent_idx in self.agents_idx:\n",
    "            reward = 0\n",
    "            # ===== 我的reward逻辑 =====\n",
    "            # if wall_collisions[agent_idx]:\n",
    "            #     reward += -10  # hitting wall penalty\n",
    "            # # else:\n",
    "            # #     reward += -1  # step cost\n",
    "\n",
    "            # if agent_idx in agents_collisions:\n",
    "            #     reward += -200  # Collision penalty\n",
    "            # else:\n",
    "            #     reward += 2  # no collision reward\n",
    "\n",
    "            # location = self.agents_positions[agent_idx]\n",
    "            next_location = next_positions[agent_idx]\n",
    "            # # distance reward to A or B\n",
    "            # goal = (\n",
    "            #     self.A_position\n",
    "            #     if not self.agents_reached_A[agent_idx]\n",
    "            #     else self.B_position\n",
    "            # )\n",
    "            # prev_dist = abs(location[0] - goal[0]) + abs(location[1] - goal[1])\n",
    "            # curr_dist = abs(next_location[0] - goal[0]) + abs(\n",
    "            #     next_location[1] - goal[1]\n",
    "            # )\n",
    "            # reward += (\n",
    "            #     max(0, (prev_dist - curr_dist)) * 5\n",
    "            # )  # every step close to goal, acculate reward by +5\n",
    "\n",
    "            # if self.agents_reached_A[agent_idx]:\n",
    "            #     # 如果之前已经取货 现在到B 就算完成一次交付\n",
    "            #     if next_location == self.B_position:\n",
    "            #         reward += 300  # delivery success\n",
    "            #         # self.agents_reached_A[agent_idx] = False\n",
    "            # else:\n",
    "            #     # 如果还没取货 现在到A 就给取货奖励\n",
    "            #     if next_location == self.A_position:\n",
    "            #         reward += 100  # pickup success\n",
    "            #         # self.agents_reached_A[agent_idx] = True\n",
    "            # ===== 我的reward逻辑 =====\n",
    "            \n",
    "\n",
    "            # 严格合规的reward逻辑\n",
    "            if self.agents_reached_A[agent_idx] == False:\n",
    "                if next_location == self.A_position:\n",
    "                    self.reached_A = True\n",
    "                    reward += 100\n",
    "                else:\n",
    "                    reward = -1\n",
    "            elif self.agents_reached_A[agent_idx] and next_location == self.B_position:\n",
    "                reward += 300\n",
    "            else:\n",
    "                reward = -1\n",
    "\n",
    "            rewards[agent_idx] = reward  # store reward\n",
    "\n",
    "        # update agents' positions\n",
    "        self.agents_positions = next_positions\n",
    "\n",
    "        # accumulate total collisions and steps\n",
    "        self.total_collisions += collisions\n",
    "        self.total_steps += self.agents_num\n",
    "\n",
    "        # Update item-carrying status after moving\n",
    "        for agent_idx in self.agents_idx:\n",
    "            if (\n",
    "                self.agents_reached_A[agent_idx]\n",
    "                and self.agents_positions[agent_idx] == self.B_position\n",
    "            ):\n",
    "                self.agents_reached_A[agent_idx] = False  # delivered item at B\n",
    "            elif (not self.agents_reached_A[agent_idx]) and self.agents_positions[\n",
    "                agent_idx\n",
    "            ] == self.A_position:\n",
    "                self.agents_reached_A[agent_idx] = True  # picked up item at A\n",
    "\n",
    "        # format next state\n",
    "        next_states = {}\n",
    "        for agent_idx in self.agents_idx:\n",
    "            next_states[agent_idx] = self.get_state(agent_idx)\n",
    "\n",
    "        # LOG\n",
    "        # print(f\"Agent {agent_idx} | Position: {next_location} | Reward: {reward}\")\n",
    "\n",
    "        return next_states, rewards, collisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9484e136",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "8d4882c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep q-learning agent\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        statespace_size,\n",
    "        action_size,\n",
    "        gamma=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        min_epsilon=0.1,\n",
    "        batch_size=256,\n",
    "        replay_buffer_size=50000,\n",
    "        lr=0.001,\n",
    "        copy_frequency=100,\n",
    "    ):\n",
    "        self.statespace_size = statespace_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.lr = lr\n",
    "        self.copy_frequency = copy_frequency\n",
    "\n",
    "        self.steps = 0  # count agent's steps\n",
    "        self.replay_buffer = []  # memory\n",
    "        self.replay_buffer_size = replay_buffer_size  # memory size\n",
    "\n",
    "        # initialize the DQN\n",
    "        self.model, self.model2, self.optimizer, self.loss_fn = self.prepare_torch()\n",
    "\n",
    "    def prepare_torch(self):\n",
    "        l1, l2, l3, l4 = self.statespace_size, 128, 128, self.action_size\n",
    "\n",
    "        model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(l1, l2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(l2, l3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(l3, l4),\n",
    "        )\n",
    "        model2 = copy.deepcopy(model)\n",
    "        model2.load_state_dict(model.state_dict())\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)\n",
    "        return model, model2, optimizer, loss_fn\n",
    "\n",
    "    def update_target(self):\n",
    "        self.model2.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        state_tensor = torch.from_numpy(state).float()\n",
    "        qvals_torch = self.model(state_tensor)\n",
    "        qvals = qvals_torch.detach().numpy()\n",
    "        return qvals\n",
    "\n",
    "    def get_maxQ(self, s):\n",
    "        return torch.max(self.model2(torch.from_numpy(s).float())).detach().numpy()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if npr.uniform() < self.epsilon:\n",
    "            action = npr.choice(self.action_size)\n",
    "        else:\n",
    "            qvals = self.get_qvals(state)\n",
    "            action = np.argmax(qvals)\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Store the transition in the replay buffer.\n",
    "        \"\"\"\n",
    "        if len(self.replay_buffer) >= self.replay_buffer_size:\n",
    "            # random remove sample\n",
    "            remove_idx = npr.randint(0, len(self.replay_buffer))\n",
    "            self.replay_buffer.pop(remove_idx)\n",
    "        self.replay_buffer.append((state, action, reward, next_state))\n",
    "\n",
    "    def train_one_step(self, states, actions, targets):\n",
    "        # convert the states and actions and targets to tensors\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        targets = np.array(targets)\n",
    "\n",
    "        state_batch = torch.tensor(states, dtype=torch.float32)\n",
    "        action_batch = torch.tensor(actions, dtype=torch.long)\n",
    "        target_batch = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "        # get Q-values for the current states\n",
    "        q_values = self.model(state_batch)\n",
    "        predicted_q_values = q_values.gather(1, action_batch.unsqueeze(1)).squeeze()\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = self.loss_fn(predicted_q_values, target_batch)\n",
    "\n",
    "        # backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the agent using the replay buffer.\n",
    "        \"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return  # samples not enough\n",
    "\n",
    "        # sample a batch from the replay buffer\n",
    "        minibatch = random.sample(\n",
    "            self.replay_buffer,\n",
    "            self.batch_size,\n",
    "        )\n",
    "        states, actions, rewards, next_states = zip(*minibatch)\n",
    "\n",
    "        # TD targets\n",
    "        targets = []\n",
    "        for i in range(len(minibatch)):\n",
    "            next_maxQ = self.get_maxQ(next_states[i])\n",
    "            action_target = rewards[i] + self.gamma * next_maxQ\n",
    "            targets.append(action_target)\n",
    "\n",
    "        # train the model\n",
    "        loss = self.train_one_step(states, actions, targets)\n",
    "\n",
    "        # update network periodically\n",
    "        self.steps += 1\n",
    "        if self.steps % self.copy_frequency == 0:\n",
    "            self.update_target()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # decay epsilon\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b27cf79",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "41721ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def train_agents(\n",
    "    agent, env, max_steps=1500000, max_collisions=4000, max_walltime=600, verbose=True\n",
    "):\n",
    "    print(\"train_agents triggered\")\n",
    "    \"\"\"Training each agent in the environment.\"\"\"\n",
    "    # start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # global variables\n",
    "    total_collisions = 0\n",
    "    total_steps = 0\n",
    "    episode = 0\n",
    "\n",
    "    while total_collisions <= max_collisions and total_steps <= max_steps:\n",
    "        if time.time() - start_time > max_walltime:\n",
    "            print(\"===== Time limit exceeded. =====\")\n",
    "            break\n",
    "\n",
    "        # episode max steps\n",
    "        steps_this_episode = 0\n",
    "\n",
    "        # initialize the environment\n",
    "        env._reset()\n",
    "\n",
    "        # initial states\n",
    "        states = {agent_idx: env.get_state(agent_idx) for agent_idx in env.agents_idx}\n",
    "\n",
    "        # episode finish flag\n",
    "        done = False\n",
    "\n",
    "        TRAIN_INTERVAL = 5  # 每 5 个环境步训练一次\n",
    "        step_since_train = 0\n",
    "\n",
    "        while not done:\n",
    "            steps_this_episode += len(env.agents_idx)\n",
    "            # force end if episode max steps exceeded. (0,0)->(4,4)\n",
    "            # max 25 steps, try set maximum 75 steps\n",
    "            if steps_this_episode >= 75 * len(env.agents_idx):\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "            actions_dict = {}\n",
    "            for agent_idx in sorted(env.agents_idx):  # central clock - fix order\n",
    "                action = agent.get_action(states[agent_idx])\n",
    "                actions_dict[agent_idx] = action\n",
    "\n",
    "            # take action in the environment\n",
    "            next_states, rewards, collisions = env.take_action(actions_dict)\n",
    "            # print(f\"[Step {total_steps + len(env.agents_idx)}] rewards: {rewards}\")\n",
    "\n",
    "            # store transition in replay buffer\n",
    "            for agent_idx in sorted(env.agents_idx):\n",
    "                state = states[agent_idx]\n",
    "                action = actions_dict[agent_idx]\n",
    "                reward = rewards[agent_idx]\n",
    "                next_state = next_states[agent_idx]\n",
    "                agent.store_transition(state, action, reward, next_state)\n",
    "\n",
    "            step_since_train += 1\n",
    "            # train the agent\n",
    "            if total_steps < 5000: # 前5000步多训练，练习避免碰撞\n",
    "                agent.train()\n",
    "            elif (\n",
    "                len(agent.replay_buffer) >= agent.batch_size\n",
    "                and step_since_train >= TRAIN_INTERVAL\n",
    "            ):\n",
    "                agent.train()\n",
    "                step_since_train = 0\n",
    "\n",
    "            total_collisions += collisions\n",
    "            total_steps += len(env.agents_idx)\n",
    "\n",
    "            # add step-based epsilon decay\n",
    "            if total_steps % 1000 == 0:\n",
    "                agent.epsilon = max(agent.min_epsilon, agent.epsilon * 0.997)\n",
    "\n",
    "            # log\n",
    "            if verbose and total_steps % 5000 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(\n",
    "                    f\"Steps: {total_steps}/{max_steps}, \"\n",
    "                    f\"Collisions: {total_collisions}/{max_collisions}, \"\n",
    "                    f\"Epsilon: {agent.epsilon:.3f}, \"\n",
    "                    f\"Time Elapsed: {elapsed:.1f}s\"\n",
    "                )\n",
    "\n",
    "            # check if any agent has done the task\n",
    "            for i in env.agents_idx:\n",
    "                if env._check_done(i):\n",
    "                    done = True\n",
    "                    break\n",
    "\n",
    "            # check if the training should stop\n",
    "            if (\n",
    "                total_steps >= max_steps\n",
    "                or total_collisions >= max_collisions\n",
    "                or time.time() - start_time > max_walltime\n",
    "            ):\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "            # update the states\n",
    "            states = next_states\n",
    "\n",
    "        # one episode end\n",
    "        episode += 1\n",
    "        if episode % 10 == 0:  # update epsilon every 10 episodes\n",
    "            agent.decay_epsilon()\n",
    "\n",
    "        # log\n",
    "        if verbose and episode % 10 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                f\"Episode: {episode}, \"\n",
    "                f\"TotalSteps: {total_steps}, \"\n",
    "                f\"TotalCollisions: {total_collisions}, \"\n",
    "                f\"Epsilon: {agent.epsilon:.3f}, \"\n",
    "                f\"Elapsed: {elapsed:.1f}s\"\n",
    "            )\n",
    "\n",
    "        # if verbose and total_steps % 5000 == 0:\n",
    "        #     elapsed_time = time.time() - start_time\n",
    "        #     print(\n",
    "        #         f\"Steps: {total_steps}/{max_steps}, Collisions: {total_collisions}/{max_collisions}, Epsilon: {agent.epsilon:.3f}, Time Elapsed: {elapsed_time:.2f}s\"\n",
    "        #     )\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "    print(f\"Total steps: {total_steps}\")\n",
    "    print(f\"Total collisions: {total_collisions}\")\n",
    "    print(f\"Final epsilon: {agent.epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c91ac5",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "02e459fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agents(agent, env, test_times=100, max_steps=25, step_verbose=True):\n",
    "    \"\"\"\n",
    "    Test the trained agent in the environment.\n",
    "    \"\"\"\n",
    "    # initialize the parmeters\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0\n",
    "    success_times = 0\n",
    "    total_steps_successful = 0\n",
    "    total_collisions = 0\n",
    "\n",
    "    for run in range(test_times):\n",
    "        # reset the environment\n",
    "        env._reset()\n",
    "\n",
    "        # all agents start at B(as the assingment requirenment says)\n",
    "        for agent_idx in env.agents_idx:\n",
    "            env.agents_positions[agent_idx] = env.B_position\n",
    "            env.agents_reached_A[agent_idx] = False\n",
    "\n",
    "        # initial states\n",
    "        states = {agent_idx: env.get_state(agent_idx) for agent_idx in env.agents_idx}\n",
    "\n",
    "        # initialize the variables\n",
    "        delivery_success = {agent_idx: False for agent_idx in env.agents_idx}\n",
    "        steps_taken = {agent_idx: 0 for agent_idx in env.agents_idx}\n",
    "        collisions_happened = False\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            actions_dict = {}\n",
    "            for agent_idx in sorted(env.agents_idx): # central clock - fix order\n",
    "                action = agent.get_action(states[agent_idx])\n",
    "                actions_dict[agent_idx] = action\n",
    "\n",
    "            # take action in the environment\n",
    "            next_states, rewards, collisions = env.take_action(actions_dict)\n",
    "            # update the states\n",
    "            states = next_states\n",
    "            # update stepts taken\n",
    "            for agent_idx in env.agents_idx:\n",
    "                steps_taken[agent_idx] += 1\n",
    "\n",
    "            # check collisions\n",
    "            if collisions > 0:\n",
    "                collisions_happened = True\n",
    "\n",
    "            # check delivery success\n",
    "            for agent_idx in env.agents_idx:\n",
    "                position = env.agents_positions[agent_idx]\n",
    "                reached_A = env.agents_reached_A[agent_idx]\n",
    "                if position == env.B_position and not reached_A:\n",
    "                    delivery_success[agent_idx] = True\n",
    "\n",
    "            # check if all agents have successfully delivered\n",
    "            if all(delivery_success.values()):\n",
    "                break\n",
    "\n",
    "\n",
    "        # record successful deliveries\n",
    "        for agent_idx in sorted(env.agents_idx):\n",
    "            if (\n",
    "                delivery_success[agent_idx]\n",
    "                and not collisions_happened\n",
    "                and steps_taken[agent_idx] <= max_steps\n",
    "            ):\n",
    "                success_times += 1\n",
    "                total_steps_successful += steps_taken[agent_idx]\n",
    "\n",
    "        # record collisions\n",
    "        if collisions_happened:\n",
    "            total_collisions += 1\n",
    "\n",
    "        # print step verbose\n",
    "        if step_verbose:\n",
    "            print(\n",
    "                f\"[TEST] Run {run + 1}/{test_times}: Success deliveries = {sum(delivery_success.values())}, Collisions: {collisions_happened}\"\n",
    "            )\n",
    "\n",
    "    # restore the epsilon\n",
    "    agent.epsilon = original_epsilon\n",
    "\n",
    "    # test indicators\n",
    "    total_possible_deliveries = len(env.agents_idx) * test_times\n",
    "    success_rate = (success_times / total_possible_deliveries) * 100\n",
    "    avg_steps = total_steps_successful / success_times if success_times > 0 else 0\n",
    "    collisions_rate = (total_collisions / test_times) * 100\n",
    "\n",
    "    # test summary\n",
    "    print(\"\\n===== Test Summary =====\")\n",
    "    print(f\"Success rate: {success_rate:.2f}%\")\n",
    "    print(f\"Average steps per successful delivery: {avg_steps:.2f}\")\n",
    "    print(f\"Total Collision: {total_collisions}\")\n",
    "\n",
    "    return {\n",
    "        \"success_rate\": success_rate,\n",
    "        \"avg_steps\": avg_steps,\n",
    "        \"total_collisions\": total_collisions,\n",
    "        \"collisions_rate\": collisions_rate,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522faa76",
   "metadata": {},
   "source": [
    "## Test process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "e7b8332a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space size: 13\n",
      "\n",
      "===== Training with steps: 500000 =====\n",
      "train_agents triggered\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'next_location' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[359], line 44\u001b[0m\n\u001b[1;32m     38\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(\n\u001b[1;32m     39\u001b[0m     statespace_size,\n\u001b[1;32m     40\u001b[0m     action_size,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# train the agent\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mtrain_agents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# test the agent\u001b[39;00m\n\u001b[1;32m     52\u001b[0m metrics \u001b[38;5;241m=\u001b[39m test_agents(\n\u001b[1;32m     53\u001b[0m     agent, env, test_times\u001b[38;5;241m=\u001b[39mtest_runs, max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, step_verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     54\u001b[0m )\n",
      "Cell \u001b[0;32mIn[357], line 51\u001b[0m, in \u001b[0;36mtrain_agents\u001b[0;34m(agent, env, max_steps, max_collisions, max_walltime, verbose)\u001b[0m\n\u001b[1;32m     48\u001b[0m     actions_dict[agent_idx] \u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# take action in the environment\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m next_states, rewards, collisions \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# print(f\"[Step {total_steps + len(env.agents_idx)}] rewards: {rewards}\")\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# store transition in replay buffer\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(env\u001b[38;5;241m.\u001b[39magents_idx):\n",
      "Cell \u001b[0;32mIn[355], line 235\u001b[0m, in \u001b[0;36mGridWorldEnvironment.take_action\u001b[0;34m(self, action_dict)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# ===== 我的reward逻辑 =====\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# if wall_collisions[agent_idx]:\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m#     reward += -10  # hitting wall penalty\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# 严格合规的reward逻辑\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents_reached_A[agent_idx] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnext_location\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA_position:\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreached_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    237\u001b[0m         reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'next_location' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "\n",
    "# setup the environment\n",
    "test_env = GridWorldEnvironment()\n",
    "test_state = test_env.get_state(0)\n",
    "statespace_size = test_state.shape[0]\n",
    "print(f\"State space size: {statespace_size}\")\n",
    "action_size = len(test_env.directions)\n",
    "\n",
    "# trainning parameters\n",
    "trainning_steps = [500000, 1000000, 1500000]\n",
    "test_runs = 100\n",
    "repeat_times = 2\n",
    "\n",
    "# results metrics\n",
    "results = {\n",
    "    \"trainning_steps\": [],\n",
    "    \"success_rate_mean\": [],\n",
    "    \"success_rate_std\": [],\n",
    "    \"average_steps_mean\": [],\n",
    "    \"average_steps_std\": [],\n",
    "    \"collision_rate_mean\": [],\n",
    "    \"collision_rate_std\": [],\n",
    "}\n",
    "\n",
    "# Test 1: trainning steps & performance\n",
    "for steps in trainning_steps:\n",
    "    print(\"\\n===== Training with steps: {} =====\".format(steps))\n",
    "\n",
    "    success_rates = []\n",
    "    average_steps_list = []\n",
    "    collision_rates = []\n",
    "\n",
    "    # initialize the agent and environment\n",
    "    env = GridWorldEnvironment()\n",
    "    agent = Agent(\n",
    "        statespace_size,\n",
    "        action_size,\n",
    "    )\n",
    "\n",
    "    # train the agent\n",
    "    train_agents(\n",
    "        agent,\n",
    "        env,\n",
    "        max_steps=steps,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # test the agent\n",
    "    metrics = test_agents(\n",
    "        agent, env, test_times=test_runs, max_steps=25, step_verbose=False\n",
    "    )\n",
    "\n",
    "    # record the results\n",
    "    success_rates.append(metrics[\"success_rate\"])\n",
    "    average_steps_list.append(metrics[\"avg_steps\"])\n",
    "    collision_rates.append(metrics[\"total_collisions\"])\n",
    "    total_collisions = metrics[\"total_collisions\"]\n",
    "\n",
    "# calculate and store the results\n",
    "results[\"trainning_steps\"].append(steps)\n",
    "results[\"success_rate_mean\"].append(np.mean(success_rates))\n",
    "results[\"success_rate_std\"].append(np.std(success_rates))\n",
    "results[\"average_steps_mean\"].append(np.mean(average_steps_list))\n",
    "results[\"average_steps_std\"].append(np.std(average_steps_list))\n",
    "# results[\"total_collisions\"].append(total_collisions)\n",
    "results[\"collision_rate_mean\"].append(np.mean(collision_rates))\n",
    "results[\"collision_rate_std\"].append(np.std(collision_rates))\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.suptitle(\"Test 1: Training Steps vs. Performance\")\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.errorbar(\n",
    "    results[\"trainning_steps\"],\n",
    "    results[\"success_rate_mean\"],\n",
    "    yerr=results[\"success_rate_std\"],\n",
    "    fmt=\"o-\",\n",
    "    capsize=5,\n",
    ")\n",
    "plt.title(\"Success Rate\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Success Rate (%)\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.errorbar(\n",
    "    results[\"trainning_steps\"],\n",
    "    results[\"average_steps_mean\"],\n",
    "    yerr=results[\"average_steps_std\"],\n",
    "    fmt=\"o-\",\n",
    "    capsize=5,\n",
    ")\n",
    "plt.title(\"Average Steps\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Average Steps\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.errorbar(\n",
    "    results[\"trainning_steps\"],\n",
    "    results[\"collision_rate_mean\"],\n",
    "    yerr=results[\"collision_rate_std\"],\n",
    "    fmt=\"o-\",\n",
    "    capsize=5,\n",
    ")\n",
    "plt.title(\"Collision Rate\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Collision Rate (%)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Test 2: Generalization\n",
    "print(\"\\n===== Generalization Test =====\")\n",
    "env = GridWorldEnvironment()\n",
    "agent = Agent(\n",
    "    statespace_size,\n",
    "    action_size,\n",
    ")\n",
    "train_agents(\n",
    "    agent,\n",
    "    env,\n",
    "    max_steps=1500000,\n",
    "    max_collisions=4000,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "metrics = test_agents(agent, env, test_times=200, max_steps=25, step_verbose=False)\n",
    "\n",
    "print(\"\\n===== Test 2 Final test metrics =====\")\n",
    "print(f\"Success rate: {metrics['success_rate']:.2f}%\")\n",
    "print(f\"Average steps: {metrics['avg_steps']:.2f}%\")\n",
    "print(f\"Total collisions: {metrics['total_collisions']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
